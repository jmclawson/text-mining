---
title: "From Page to Table"
subtitle: "Adding Structure Where It Wasn't"
categories: 
  - importing text
  - writing functions
order: 3
df-print: default
description: "Introducing approaches for reading text files, using Grambling's alma maters as a case study."
tutorials: 
  - href: "http://jmclawson.shinyapps.io/ltm-05-structure/"
    title: "05 - Adding Structure to Unstructured Text"
draft: false
freeze: true
---

There are certain considerations to keep in mind when working with text in any kind of analysis. Text is often called "unstructured data" because it initially resists techniques that typically apply to studying other kinds of data. With a little effort, we can add structure by reading text data into a table structure. This lesson shows one such process.

## Getting started

As always, it's a good idea to start by loading packages. We'll usually build on packages in the `tidyverse` set of packages, so let's start there. Later on, we'll also be using the `tidytext` package for *tokenizing* the text (defining what constitutes a word):

```{r}
#| message: false
library(tidyverse)
library(tidytext)
```

That's all we'll need in this lesson. Later lessons will use more.

## Reading a text file

Our `data/` folder has a text file for each member institution of the Southwestern Athletic Conference (or SWAC). We'll start by reading Grambling's alma mater and work through preparing it for study. Perhaps the easiest way to read a text file is to use the `readLines()` function to read the file line by line. For this function, the only argument needed is a file name.

```{r}
readLines("data/almamater_gram.txt")
```

Notice here that the ninth line is blank. This is an empty line between stanzas, and it'll come in handy in the next section.

## Adding structure

When we talk about structured data, we typically mean data recorded in a table of rows and columns. For Grambling's alma mater, we might imagine a table with one row per line of text, with added columns indicating which school or university, the stanza number, and the line number. Let's start with a table of the first two columns and add stanza and line numbers in subsequent steps:

```{r}
alma_mater <- data.frame(
  school = "gram",
  text = readLines("data/almamater_gram.txt"))

alma_mater
```

### Adding stanza markers

As noted previously, the ninth line here is blank, which is a helpful way to recognize a new stanza. Since there's only two stanzas here, we *could* define things manually:

```{r}
alma_mater |> 
  mutate(stanza_num = c(1, 1, 1, 1, 1, 1, 1, 1, NA, 2, 2, 2, 2, 2, 2, 2, 2))
```

But doing things manually like this is a mess. It introduces room for mistakes, it isn't easy, and it gets even tougher when we have longer poems. For this purpose, the `cumsum()` function---for "cumulative sum"---can be a good way of adding up all the spaces. Using a logical test inside the parenthesis means it'll add an increment for each time it's true:

```{r}
alma_mater |> 
  mutate(
    stanza_num = cumsum(text == ""))
```

Here, the test inside `cumsum()` matches every time the `text` column is equal to "" (or a blank), so it increments a counter in the `stanza_num` column. And it's almost perfect! The only problem is that it should start at *one* instead of *zero*. That's easy to fix by adding 1. While we're at it, we'll exclude the blank line using `filter()`, since it doesn't add any data.

```{r}
alma_mater <- 
  alma_mater |> 
  mutate(stanza_num = cumsum(text == "") + 1) |> 
  filter(text != "")

alma_mater
```

### Adding line markers

Line numbers are even easier to add using the `row_number()` function. Here, we use it with `mutate()`, and we'll move the metadata columns before the text column using `relocate()`:

```{r}
alma_mater <- 
  alma_mater |> 
  mutate(line_num = row_number()) |> 
  relocate(stanza_num, line_num,
           .before = text)

alma_mater
```

## Making it tidy

Organizing the lyrics by line makes sense in some circumstances, but it's also reasonable to read the lyrics on a word-by-word basis. For this reason, some may even argue that a row-per-word is a "tidier" structure.

When we think about splitting up by words, it's important to keep in mind not only the ways we recognize word boundaries, but also what we're after:

1.  We might assume spaces are sufficient to define word boundaries. In some circumstances---such as this sentence, which contains an em-dash interruption---spaces may not be sufficient to show word boundaries.
2.  Breaking up by spaces will also keep punctuation marks like commas and periods connected to words.
3.  Simply splitting by spaces keeps capitalization. Is a given word capitalized because it's a proper noun (and thus the capitalization is part of its spelling) or because of where it falls in the sentence (at the beginning)? When we remove the context of sentences, capitalization is more confusing than revealing, so we'll typically want to normalize letter case when splitting things up by words.

Lucky for us, the `tidytext` package provides a handy one-step function `unnest_tokens()` to hit all three marks, standardizing letters into lower case and separating words at spaces and punctuation.

```{r}
alma_mater |> 
  unnest_tokens(word, text) |> 
  head()
```

::: callout-note
This function's name introduces the term *token*, related to the word *tokenizing* which means the splitting of a longer text into the smallest pieces used in analysis. When we're considering frequency of words, our tokens will be the words themselves, and our process of tokenizing solidifies certain decisions of capitalization (that it is a distraction) and punctuation (that it is irrelevant) appropriate to that method.
:::

## Making a function

Putting it all together into a single code chunk, we can see that the whole thing is very reproducible. Only the first couple lines would change if we were going to apply it to the lyrics of a different university's alma mater:

```{r}
alma_mater <- 
  tibble(school = "gram",
         text = readLines("data/almamater_gram.txt")) |> 
  mutate(stanza_num = cumsum(text == "") + 1) |> 
  filter(text != "") |> 
  mutate(line_num = row_number()) |> 
  unnest_tokens(word, text) |> 
  relocate(stanza_num, line_num,
           .before = word) 

alma_mater
```

Now that we've figured out the necessary steps, it is easy to turn it into a function for converting any poem's text file into a tidy data structure. The only argument this function needs is the abbreviation of the school:

```{r}
tidy_alma <- function(school){
  file <- paste0("data/almamater_",
                 school,
                 ".txt")
  
  tibble(school = school,
         text = readLines(file)) |> 
  mutate(stanza_num = cumsum(text == "") + 1) |> 
  filter(text != "") |> 
  mutate(line_num = row_number()) |> 
  unnest_tokens(word, text) |> 
  relocate(stanza_num, line_num,
           .before = word) 
}
```

From here, a single function call will get us the tidied alma mater of any school in our folder:

```{r}
tidy_alma("uapb")
tidy_alma("subr")
tidy_alma("jsums")
```

From here, we can do clever things with our structured data.

But that's for another lesson.
